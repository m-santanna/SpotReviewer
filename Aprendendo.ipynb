{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import imageio\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask(mask):\n",
    "    # Thresholding (Otsu's method)\n",
    "\n",
    "    mask = mask.astype(np.uint8)\n",
    "    _, binary_mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "\n",
    "    # Binary dilation\n",
    "    dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "\n",
    "    # Border clearing\n",
    "    cleared_mask = dilated_mask.copy()\n",
    "    contours, _ = cv2.findContours(cleared_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(cleared_mask, contours, -1, 1, thickness=cv2.FILLED)\n",
    "\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    middle_part_mask = np.zeros_like(cleared_mask)\n",
    "    cv2.drawContours(middle_part_mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    smoothed_mask = cv2.erode(middle_part_mask, kernel, iterations=1)\n",
    "    smoothed_mask = gaussian_filter(smoothed_mask.astype(float), sigma=1)\n",
    "    cleared_mask = smoothed_mask\n",
    "\n",
    "    # Small object removal\n",
    "    cleared_mask = cv2.morphologyEx(cleared_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Hole-filling within the lesion masks\n",
    "    cleared_mask = cv2.morphologyEx(cleared_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return cleared_mask.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circularity_f(processed_mask):\n",
    "    label_image = measure.label(processed_mask, connectivity=2)\n",
    "    properties = measure.regionprops(label_image)\n",
    "    largest_region = max(properties, key=lambda x: x.area)\n",
    "    circularity = 4 * np.pi * largest_region.area / (largest_region.perimeter ** 2)\n",
    "    return circularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory_test = \"C:\\\\Dev\\\\Soft\\\\test\"\n",
    "working_directory_train = \"C:\\\\Dev\\\\Soft\\\\train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Set your parameters\n",
    "target_size = (100, 100)\n",
    "batch_size = 50\n",
    "consistent_length = None\n",
    "image_tensors = []\n",
    "circularity_tensors = []\n",
    "\n",
    "image_files = [f for f in os.listdir(working_directory_train) if f.endswith('.JPG')]\n",
    "size = len(image_files)\n",
    "\n",
    "for i in range(0, len(image_files), batch_size):\n",
    "    batch = image_files[i:i + batch_size]\n",
    "    print(i)\n",
    "    batch_images = []\n",
    "    circularity_vector = []\n",
    "\n",
    "    for file in batch:\n",
    "        image = Image.open(os.path.join(working_directory_train, file))\n",
    "        resized_image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        gray_image = 255 - resized_image_array.mean(axis=2).astype('int32')\n",
    "        processed_mask = process_mask(gray_image)\n",
    "        final_masked_image = cv2.bitwise_and(gray_image,gray_image,mask=processed_mask)\n",
    "        final_masked_resized = cv2.resize(final_masked_image.astype('uint8'), target_size)\n",
    "        batch_images.append(final_masked_resized)\n",
    "      \n",
    "\n",
    "    # Convert the batch of images to a tensor\n",
    "    batch_tensor = np.stack(batch_images, axis=0)\n",
    "    image_tensors.append(batch_tensor)\n",
    "    \n",
    "    \n",
    "final_tensor = np.vstack(image_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "image_tensors_test = []\n",
    "circularity_tensors_test = []\n",
    "image_files_test = [f for f in os.listdir(working_directory_test) if f.endswith('.JPG')]\n",
    "\n",
    "for i in range(0, len(image_files_test), batch_size):\n",
    "    batch_test = image_files_test[i:i + batch_size]\n",
    "    print(i)\n",
    "    batch_images_test = []\n",
    "    circularity_vector_test = []\n",
    "\n",
    "    for files in batch_test:\n",
    "        image_test = Image.open(os.path.join(working_directory_test, files))\n",
    "        resized_image_array_test = tf.keras.preprocessing.image.img_to_array(image_test)\n",
    "        gray_image_test = 255 - resized_image_array_test.mean(axis=2).astype('int32')\n",
    "        processed_mask_test = process_mask(gray_image_test)\n",
    "        final_masked_image_test = cv2.bitwise_and(gray_image_test,gray_image_test,mask=processed_mask_test)\n",
    "        final_masked_resized_test = cv2.resize(final_masked_image_test.astype('uint8'), target_size)\n",
    "        batch_images_test.append(final_masked_resized_test)\n",
    "\n",
    "    batch_tensor_test = np.stack(batch_images_test, axis=0)\n",
    "    image_tensors_test.append(batch_tensor_test)\n",
    "\n",
    "final_tensor_test = np.vstack(image_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opens the excel file and creates a tensor of the classes\n",
    "\n",
    "with open(\"train_class.csv\", mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    header = next(csv_reader)\n",
    "    age_column_index = header.index(\"benign_malignant\")\n",
    "    age_data = [row[age_column_index] for row in csv_reader]\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "class_tensor_np = np.array(age_data) \n",
    "\n",
    "desired_classes = ['benign', 'malignant']\n",
    "filter_condition = np.isin(age_data, desired_classes)\n",
    "filtered_classes = class_tensor_np[filter_condition]\n",
    "class_tensor_encoded_np = label_encoder.fit_transform(filtered_classes)\n",
    "class_tensor_encoded = tf.constant(class_tensor_encoded_np, dtype=tf.int32)\n",
    "\n",
    "filtered_tensor1 = final_tensor[filter_condition]\n",
    "\n",
    "#filtered_circularity_tensor = final_circularity[filter_condition]\n",
    "#circularity_tensor = tf.constant(filtered_circularity_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opens the excel file and creates a tensor of the classes for the test images\n",
    "\n",
    "with open(\"test_class.csv\", mode='r') as file:\n",
    "    csv_reader_test = csv.reader(file)\n",
    "    header_test = next(csv_reader_test)\n",
    "    age_column_index_test = header_test.index(\"benign_malignant\")\n",
    "    age_data_test = [row[age_column_index_test] for row in csv_reader_test]\n",
    "\n",
    "\n",
    "\n",
    "label_encoder_test = LabelEncoder()\n",
    "class_tensor_np_test = np.array(age_data_test)\n",
    "\n",
    "desired_classes = ['benign', 'malignant']\n",
    "filter_condition_test = np.isin(age_data_test, desired_classes)\n",
    "\n",
    "filtered_classes_test = class_tensor_np_test[filter_condition_test]\n",
    "class_tensor_encoded_np_test = label_encoder_test.fit_transform(filtered_classes_test)\n",
    "class_tensor_encoded_test = tf.constant(class_tensor_encoded_np_test, dtype=tf.int32)\n",
    "\n",
    "filtered_tensor1_test = final_tensor_test[filter_condition_test]\n",
    "\n",
    "#filtered_circularity_tensor_test = final_circularity[filter_condition_test]\n",
    "#circularity_tensor_test = tf.constant(filtered_circularity_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = filtered_tensor1\n",
    "y_train = class_tensor_encoded\n",
    "\n",
    "X_test1 = filtered_tensor1_test\n",
    "y_test = class_tensor_encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 17s 333ms/step - loss: 2.5763 - accuracy: 0.8334 - val_loss: 1.8580 - val_accuracy: 0.7199\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 17s 338ms/step - loss: 1.3777 - accuracy: 0.8809 - val_loss: 1.8078 - val_accuracy: 0.7244\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 16s 334ms/step - loss: 1.3346 - accuracy: 0.8894 - val_loss: 1.8035 - val_accuracy: 0.7231\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 17s 348ms/step - loss: 1.3111 - accuracy: 0.8970 - val_loss: 1.8195 - val_accuracy: 0.7238\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 17s 347ms/step - loss: 1.2948 - accuracy: 0.8951 - val_loss: 1.7607 - val_accuracy: 0.7199\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 16s 332ms/step - loss: 1.2757 - accuracy: 0.9004 - val_loss: 1.8267 - val_accuracy: 0.7161\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 16s 335ms/step - loss: 1.2557 - accuracy: 0.9052 - val_loss: 1.8213 - val_accuracy: 0.7180\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 16s 334ms/step - loss: 1.2372 - accuracy: 0.9154 - val_loss: 1.8614 - val_accuracy: 0.7219\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 16s 328ms/step - loss: 1.2294 - accuracy: 0.9132 - val_loss: 1.8931 - val_accuracy: 0.7148\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 17s 349ms/step - loss: 1.2144 - accuracy: 0.9209 - val_loss: 1.8933 - val_accuracy: 0.7136\n",
      "34/34 [==============================] - 1s 21ms/step - loss: 1.4996 - accuracy: 0.8407\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-4\n",
    "n_iterations = 501\n",
    "batch_size = 128\n",
    "dropout = 0.5\n",
    "l2_reg = 1e-3 \n",
    "\n",
    "input_shape = (100, 100, 1)\n",
    "\n",
    "model1 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model1.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.fit(X_train1, y_train, epochs=10, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model1.evaluate(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 21ms/step\n",
      "Confusion Matrix:\n",
      "[[903   6]\n",
      " [166   5]]\n",
      "0.054945054945054944 0.029239766081871343 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model1.predict(X_test1)\n",
    "\n",
    "# Convert predicted probabilities to predicted labels\n",
    "y_pred_labels = np.argmax(y_pred_prob, axis = 1)\n",
    "\n",
    "# Convert one-hot encoded true labels to integer labels\n",
    "y_true_labels = np.array(y_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "precision = precision_score(y_true_labels, y_pred_labels)\n",
    "recall = recall_score(y_true_labels, y_pred_labels)\n",
    "f1 = f1_score(y_true_labels, y_pred_labels)\n",
    "\n",
    "print(f1,recall,precision)\n",
    "float_tensor = tf.cast(X_test1, tf.float32)\n",
    "std_deviation = tf.math.reduce_std(float_tensor, axis=[1, 2])\n",
    "concatenated_vector = np.column_stack((y_pred_prob,std_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "# Fit the KMeans model to the data\n",
    "kmeans.fit(concatenated_vector)\n",
    "\n",
    "# Get cluster labels and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "comparing = np.column_stack((y_true_labels,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# Assuming 'true_labels' is your ground truth vector and 'predicted_labels' is the K-Means cluster assignments\n",
    "nmi = normalized_mutual_info_score(y_true_labels, labels)\n",
    "print(\"Normalized Mutual Information:\", nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for each cluster\n",
    "for i in range(kmeans.n_clusters):\n",
    "    ax.scatter(concatenated_vector[labels == i, 0], concatenated_vector[labels == i, 1], concatenated_vector[labels == i, 2], label=f'Cluster {i + 1}')\n",
    "\n",
    "# Plot cluster centers\n",
    "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2],\n",
    "           s=300, c='red', marker='X', label='Cluster Centers')\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "ax.set_title('3D Clustering Visualization')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenated_vector[:, 0]\n",
    "y = concatenated_vector[:, 1]\n",
    "z = concatenated_vector[:, 2]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, c=y_true_labels, marker='o')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = concatenated_vector[:, 0]\n",
    "z = concatenated_vector[:, 1]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "\n",
    "plt.scatter(x, z, c=y_true_labels, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[558 351]\n",
      " [ 72  99]]\n",
      "0.22\n"
     ]
    }
   ],
   "source": [
    "tetha1 = 0.06\n",
    "tetha2 = 30\n",
    "\n",
    "pred_f = []\n",
    "\n",
    "for s in concatenated_vector:\n",
    "    \n",
    "    if s[1] > tetha1 and s[2] > tetha2:\n",
    "        pred_f += [1]\n",
    " \n",
    "    else:\n",
    "        pred_f += [0]\n",
    "\n",
    "conf_matrix_f = confusion_matrix(y_true_labels, pred_f)\n",
    "conc = np.column_stack((y_true_labels,pred_f))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_f)\n",
    "precision = precision_score(y_true_labels, pred_f)\n",
    "print(precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
