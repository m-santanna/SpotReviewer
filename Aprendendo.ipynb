{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import imageio\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mask(mask):\n",
    "    # Thresholding (Otsu's method)\n",
    "\n",
    "    mask = mask.astype(np.uint8)\n",
    "    _, binary_mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Morphological operations\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "\n",
    "    # Binary dilation\n",
    "    dilated_mask = cv2.dilate(binary_mask, kernel, iterations=1)\n",
    "\n",
    "    # Border clearing\n",
    "    cleared_mask = dilated_mask.copy()\n",
    "    contours, _ = cv2.findContours(cleared_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(cleared_mask, contours, -1, 1, thickness=cv2.FILLED)\n",
    "\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    middle_part_mask = np.zeros_like(cleared_mask)\n",
    "    cv2.drawContours(middle_part_mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    smoothed_mask = cv2.erode(middle_part_mask, kernel, iterations=1)\n",
    "    smoothed_mask = gaussian_filter(smoothed_mask.astype(float), sigma=1)\n",
    "    cleared_mask = smoothed_mask\n",
    "\n",
    "    # Small object removal\n",
    "    cleared_mask = cv2.morphologyEx(cleared_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Hole-filling within the lesion masks\n",
    "    cleared_mask = cv2.morphologyEx(cleared_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return cleared_mask.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circularity_f(processed_mask):\n",
    "    label_image = measure.label(processed_mask, connectivity=2)\n",
    "    properties = measure.regionprops(label_image)\n",
    "    largest_region = max(properties, key=lambda x: x.area)\n",
    "    circularity = 4 * np.pi * largest_region.area / (largest_region.perimeter ** 2)\n",
    "    return circularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = np.reshape(image_1d[7], target_size)\n",
    "processed_mask = process_mask(original_image)\n",
    "\n",
    "final_masked_image = cv2.bitwise_and(original_image,original_image,mask=processed_mask)\n",
    "\n",
    "# Compute the standard deviation of the intensity distribution for each lesion\n",
    "std_deviation = np.std(final_masked_image)\n",
    "\n",
    "# Use region props to obtain the circularity of each mask\n",
    "label_image = measure.label(processed_mask, connectivity=2)\n",
    "\n",
    "properties = measure.regionprops(label_image)\n",
    "largest_region = max(properties, key=lambda x: x.area)\n",
    "\n",
    "# Calculate circularity for the largest region\n",
    "circularity = 4 * np.pi * largest_region.area / (largest_region.perimeter ** 2)\n",
    "\n",
    "print(f\"Lesion Circularity: {circularity}\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original_image, cmap='gray')\n",
    "plt.title('Processed Mask')\n",
    "\n",
    "# Display the original image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(final_masked_image, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "\n",
    "# Display the processed mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(processed_mask, cmap='gray')\n",
    "plt.title('Processed Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.7\n",
    "# Get the total number of samples\n",
    "num_samples = tf.shape(class_tensor_encoded)[0]\n",
    "\n",
    "num_train_samples = tf.cast(tf.floor(split_ratio * tf.cast(num_samples, tf.float32)), tf.int32)\n",
    "num_test_samples = num_samples - num_train_samples\n",
    "\n",
    "# Generate random indices for shuffling\n",
    "indices = tf.range(num_samples)\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "# Split the indices into training and testing sets\n",
    "train_indices = shuffled_indices[:num_train_samples]\n",
    "test_indices = shuffled_indices[num_train_samples:]\n",
    "\n",
    "# Use tf.gather to get the training and testing sets\n",
    "X_train = tf.gather(filtered_values, train_indices)\n",
    "y_train = tf.gather(class_tensor_encoded, train_indices)\n",
    "circularity_test = tf.gather(circularity_tensor,test_indices)\n",
    "X_test = tf.gather(filtered_values, test_indices)\n",
    "y_test = tf.gather(class_tensor_encoded, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory_test = \"/Users/matheusss03/Developer/appDosGuri/test\"\n",
    "working_directory_train = \"/Users/matheusss03/Developer/appDosGuri/training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your parameters\n",
    "target_size = (100, 100)\n",
    "batch_size = 50\n",
    "consistent_length = None\n",
    "image_tensors = []\n",
    "circularity_tensors = []\n",
    "\n",
    "image_files = [f for f in os.listdir(working_directory_train) if f.endswith('.JPG')]\n",
    "size = len(image_files)\n",
    "\n",
    "for i in range(0, len(image_files), batch_size):\n",
    "    batch = image_files[i:i + batch_size]\n",
    "    print(i)\n",
    "    batch_images = []\n",
    "    circularity_vector = []\n",
    "\n",
    "    for file in batch:\n",
    "        image = Image.open(os.path.join(working_directory_train, file))\n",
    "        resized_image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        gray_image = 255 - resized_image_array.mean(axis=2).astype('int32')\n",
    "        processed_mask = process_mask(gray_image)\n",
    "        final_masked_image = cv2.bitwise_and(gray_image,gray_image,mask=processed_mask)\n",
    "        final_masked_resized = cv2.resize(final_masked_image.astype('uint8'), target_size)\n",
    "        batch_images.append(final_masked_resized)\n",
    "        circularity_vector.append(circularity_f(processed_mask))\n",
    "        \n",
    "\n",
    "    # Convert the batch of images to a tensor\n",
    "    batch_tensor = np.stack(batch_images, axis=0)\n",
    "    image_tensors.append(batch_tensor)\n",
    "    circularity_tensors.append(circularity_vector)\n",
    "    \n",
    "# Stack the list of tensors to create the final tensor\n",
    "final_tensor = np.vstack(image_tensors)\n",
    "final_circularity = np.vstack(circularity_tensors)\n",
    "filtered_circularity_tensor = final_circularity[filter_condition]\n",
    "filtered_tensor1 = final_tensor[filter_condition]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensors_test = []\n",
    "circularity_tensors_test = []\n",
    "\n",
    "image_files_test = [f for f in os.listdir(working_directory_test) if f.endswith('.JPG')]\n",
    "size = len(image_files)\n",
    "\n",
    "for i in range(0, len(image_files_test), batch_size):\n",
    "    batch_test = image_files_test[i:i + batch_size]\n",
    "    print(i)\n",
    "    batch_images_test = []\n",
    "    circularity_vector_test = []\n",
    "\n",
    "    for file in batch:\n",
    "        image_test = Image.open(os.path.join(working_directory_test, file_test))\n",
    "        resized_image_array_test = tf.keras.preprocessing.image.img_to_array(image_test)\n",
    "        gray_image_test = 255 - resized_image_array_test.mean(axis=2).astype('int32')\n",
    "        processed_mask_test = process_mask(gray_image_test)\n",
    "        final_masked_image_test = cv2.bitwise_and(gray_image_test,gray_image_test,mask=processed_mask_test)\n",
    "        final_masked_resized_test = cv2.resize(final_masked_image_test.astype('uint8'), target_size)\n",
    "        batch_images_test.append(final_masked_resized_test)\n",
    "        circularity_vector_test.append(circularity_f(processed_mask_test))\n",
    "        \n",
    "\n",
    "    # Convert the batch of images to a tensor\n",
    "    batch_tensor_test = np.stack(batch_images_test, axis=0)\n",
    "    \n",
    "    image_tensors_test.append(batch_tensor_test)\n",
    "    circularity_tensors_test.append(circularity_vector_test)\n",
    "    \n",
    "# Stack the list of tensors to create the final tensor\n",
    "final_tensor_test = np.vstack(image_tensors_test)\n",
    "final_circularity_test = np.vstack(circularity_tensors_test)\n",
    "filtered_circularity_tensor_test = final_circularity[filter_condition_test]\n",
    "filtered_tensor1_test = final_tensor[filter_condition_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opens the excel file and creates a tensor of the classes\n",
    "\n",
    "with open(\"challenge-2016-training_metadata_2023-11-04.csv\", mode='r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    header = next(csv_reader)\n",
    "    age_column_index = header.index(\"benign_malignant\")\n",
    "    age_data = [row[age_column_index] for row in csv_reader]\n",
    "\n",
    "image_vectors_tensor = tf.stack(image_1d)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "class_tensor_np = np.array(age_data) \n",
    "\n",
    "desired_classes = ['benign', 'malignant']\n",
    "filter_condition = np.isin(age_data, desired_classes)\n",
    "\n",
    "filtered_values = image_vectors_tensor[filter_condition]\n",
    "filtered_classes = class_tensor_np[filter_condition]\n",
    "\n",
    "circularity_vector1 = np.array(circularity_vector)\n",
    "circularity_vector_filtered = circularity_vector1[filter_condition]\n",
    "circularity_tensor = tf.constant(circularity_vector_filtered)\n",
    "\n",
    "\n",
    "class_tensor_encoded_np = label_encoder.fit_transform(filtered_classes)\n",
    "class_tensor_encoded = tf.constant(class_tensor_encoded_np, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Opens the excel file and creates a tensor of the classes for the test images\n",
    "\n",
    "with open(\"challenge-2016-training_metadata_2023-11-04.csv\", mode='r') as file:\n",
    "    csv_reader_test = csv.reader(file)\n",
    "    header_test = next(csv_reader_test)\n",
    "    age_column_index_test = header_test.index(\"benign_malignant\")\n",
    "    age_data_test = [row[age_column_index_test] for row in csv_reader_test]\n",
    "\n",
    "image_vectors_tensor_test = tf.stack(image_1d_test)\n",
    "\n",
    "label_encoder_test = LabelEncoder()\n",
    "class_tensor_np_test = np.array(age_data_test) \n",
    "\n",
    "desired_classes = ['benign', 'malignant']\n",
    "filter_condition_test = np.isin(age_data_test, desired_classes)\n",
    "\n",
    "filtered_values_test = image_vectors_tensor_test[filter_condition_test]\n",
    "filtered_classes_test = class_tensor_np_test[filter_condition_test]\n",
    "\n",
    "circularity_vector1_test = np.array(circularity_vector_test)\n",
    "circularity_vector_filtered_test = circularity_vector1_test[filter_condition_test]\n",
    "circularity_tensor_test = tf.constant(circularity_vector_filtered_test)\n",
    "\n",
    "\n",
    "class_tensor_encoded_np_test = label_encoder_test.fit_transform(filtered_classes_test)\n",
    "class_tensor_encoded_test = tf.constant(class_tensor_encoded_np_test, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices((filtered_tensor1, class_tensor_encoded))\n",
    "\n",
    "split_ratio = 0.7\n",
    "# Get the total number of samples\n",
    "num_samples = tf.shape(class_tensor_encoded)[0]\n",
    "\n",
    "num_train_samples = tf.cast(tf.floor(split_ratio * tf.cast(num_samples, tf.float32)), tf.int32)\n",
    "num_test_samples = num_samples - num_train_samples\n",
    "\n",
    "# Generate random indices for shuffling\n",
    "indices = tf.range(num_samples)\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "# Split the indices into training and testing sets\n",
    "train_indices = shuffled_indices[:num_train_samples]\n",
    "test_indices = shuffled_indices[num_train_samples:]\n",
    "\n",
    "# Use tf.gather to get the training and testing sets\n",
    "X_train1 = tf.gather(filtered_tensor1, train_indices)\n",
    "y_train = tf.gather(class_tensor_encoded, train_indices)\n",
    "\n",
    "X_test1 = tf.gather(filtered_tensor1, test_indices)\n",
    "\n",
    "y_test = tf.gather(class_tensor_encoded, test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 1)\n",
    "\n",
    "model1 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
    "    layers.Dropout(dropout),\n",
    "    layers.Dense(2, activation='softmax')  # Binary classification, use 'softmax' for multi-class\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model1.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model1.fit(X_train1, y_train, epochs=30, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model1.evaluate(X_test1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model1.predict(X_test1)\n",
    "\n",
    "# Convert predicted probabilities to predicted labels\n",
    "y_pred_labels = np.argmax(y_pred_prob, axis = 1)\n",
    "\n",
    "# Convert one-hot encoded true labels to integer labels\n",
    "y_true_labels = np.array(y_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "precision = precision_score(y_true_labels, y_pred_labels)\n",
    "recall = recall_score(y_true_labels, y_pred_labels)\n",
    "f1 = f1_score(y_true_labels, y_pred_labels)\n",
    "\n",
    "print(f1,recall,precision)\n",
    "float_tensor = tf.cast(X_test1, tf.float32)\n",
    "std_deviation = tf.math.reduce_std(float_tensor, axis=[1, 2])\n",
    "concatenated_vector = np.column_stack((y_pred_prob,circularity_test,std_deviation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "# Fit the KMeans model to the data\n",
    "kmeans.fit(concatenated_vector)\n",
    "\n",
    "# Get cluster labels and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "comparing = np.column_stack((y_true_labels,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# Assuming 'true_labels' is your ground truth vector and 'predicted_labels' is the K-Means cluster assignments\n",
    "nmi = normalized_mutual_info_score(y_true_labels, labels)\n",
    "print(\"Normalized Mutual Information:\", nmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot for each cluster\n",
    "for i in range(kmeans.n_clusters):\n",
    "    ax.scatter(concatenated_vector[labels == i, 0], concatenated_vector[labels == i, 1], concatenated_vector[labels == i, 2], label=f'Cluster {i + 1}')\n",
    "\n",
    "# Plot cluster centers\n",
    "ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2],\n",
    "           s=300, c='red', marker='X', label='Cluster Centers')\n",
    "\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.set_zlabel('Feature 3')\n",
    "ax.set_title('3D Clustering Visualization')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenated_vector[:, 0]\n",
    "y = concatenated_vector[:, 1]\n",
    "z = concatenated_vector[:, 2]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x, y, z, c=y_true_labels, marker='o')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = concatenated_vector[:, 0]\n",
    "z = concatenated_vector[:, 1]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "\n",
    "plt.scatter(x, z, c=y_true_labels, marker='o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetha1 = 0.01\n",
    "tetha2 = 30\n",
    "\n",
    "pred_f = []\n",
    "\n",
    "for s in concatenated_vector:\n",
    "    \n",
    "    if s[1] > tetha1 and s[3] > tetha2 and s[1] < 0.9999:\n",
    "        pred_f += [1]\n",
    " \n",
    "    else:\n",
    "        pred_f += [0]\n",
    "\n",
    "conf_matrix_f = confusion_matrix(y_true_labels, pred_f)\n",
    "conc = np.column_stack((y_true_labels,pred_f))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_f)\n",
    "precision = precision_score(y_true_labels, pred_f)\n",
    "print(precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
